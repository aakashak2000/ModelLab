# ModelLab: Evolution of Text Generation Models

<p align="center">
  <img src="https://raw.githubusercontent.com/aakashak2000/ModelLab/master/ModelLab_logo.png" alt="ModelLab Logo" width="300"/>
</p>

ModelLab is a comprehensive project exploring the evolution of neural text generation architectures, from simple RNNs to complex transformer-based models. This repository provides implementations, training pipelines, and analysis tools to understand how different architectures approach the challenge of text generation.

<p align="center">
  <img src="https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white" alt="PyTorch"/>
  <img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python"/>
</p>
<p align="center">
  <img src="https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge" alt="License: MIT"/>
  <img src="https://img.shields.io/github/stars/aakashak2000/ModelLab.svg?style=for-the-badge" alt="Stars"/>
  <img src="https://img.shields.io/github/forks/aakashak2000/ModelLab.svg?style=for-the-badge" alt="Forks"/>
</p>

## ğŸš€ Project Overview

ModelLab demonstrates the progressive improvement of neural language models through the implementation of increasingly sophisticated architectures:

- **Basic RNN**: Simple recurrent networks with limited memory capabilities
- **LSTM**: Long Short-Term Memory networks that better handle long-range dependencies 
- **GRU**: Gated Recurrent Units, a more efficient alternative to LSTMs âœ“
- **Seq2Seq**: Encoder-decoder architecture with partially implemented encoder-decoder blocks
- **Attention**: Models with attention mechanisms for better context awareness (WIP)
- **Transformer**: Modern architecture based on self-attention mechanisms (WIP)
- **MiniGPT**: Lightweight implementation of GPT-like architectures (WIP)

Each architecture is implemented both with PyTorch's built-in modules and from scratch to provide deeper insights into their inner workings.

## ğŸ“‚ Project Structure

```
ModelLab/
â”œâ”€â”€ src/                   # Main source code
â”‚   â”œâ”€â”€ architectures/     # Neural network architectures
â”‚   â”‚   â”œâ”€â”€ rnn/           # RNN implementations
â”‚   â”‚   â”œâ”€â”€ lstm/          # LSTM implementations
â”‚   â”‚   â”œâ”€â”€ gru/           # GRU implementations
â”‚   â”‚   â”œâ”€â”€ seq2seq/       # Sequence-to-sequence (encoder-decoder implemented)
â”‚   â”‚   â”œâ”€â”€ attention/     # Attention mechanisms (WIP)
â”‚   â”‚   â”œâ”€â”€ transformer/   # Transformer architecture (WIP)
â”‚   â”‚   â””â”€â”€ miniGPT/       # GPT-like model (WIP)
â”‚   â”œâ”€â”€ models/            # Model training and interfaces
â”‚   â”œâ”€â”€ data/              # Data loading and processing
â”‚   â”œâ”€â”€ utils/             # Utility functions
â”‚   â””â”€â”€ scripts/           # Training and generation scripts
â”œâ”€â”€ notebooks/             # Jupyter notebooks for exploration (coming soon)
â”œâ”€â”€ tests/                 # Unit tests
â”œâ”€â”€ data/                  # Raw and processed datasets
â””â”€â”€ results/               # Generated samples and metrics
```

## Development Status

| Architecture | PyTorch Implementation | From-Scratch Implementation | Status |
|--------------|:----------------------:|:---------------------------:|:------:|
| RNN          | âœ…                     | â³                          | Active |
| LSTM         | âœ…                     | â³                          | Active |
| GRU          | âœ…                     | â³                          | Active |
| Seq2Seq      | ğŸ”„                     | â³                          | In Progress |
| Attention    | â³                     | â³                          | Planned |
| Transformer  | â³                     | â³                          | Planned |
| MiniGPT      | â³                     | â³                          | Planned |

Legend:
- âœ… Complete
- ğŸ”„ Partially Implemented
- â³ Planned

## ğŸ”§ Setup and Usage

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/ModelLab.git
cd ModelLab

# Create a virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Training a Model

```bash
# Basic RNN model training
python src/scripts/train.py --model rnn --data-file data/sample_text.txt --char-level --num-epochs 10

# LSTM model with custom parameters
python src/scripts/train.py \
    --model lstm \
    --data-file data/sample_text.txt \
    --char-level \
    --seq-length 128 \
    --batch-size 64 \
    --embedding-dim 256 \
    --hidden-dim 512 \
    --num-layers 2 \
    --dropout 0.3 \
    --num-epochs 20
```

### Generating Text

```bash
# Generate text using a trained model
python src/scripts/generate.py \
    --model lstm \
    --model-path src/models/lstm_best.pt \
    --seed-text "Once upon a time" \
    --max-length 500 \
    --temperature 0.8
```

## ğŸ“ˆ Model Comparison

The repository will include analysis tools to compare different architectures across metrics like:

- Training and validation loss
- Perplexity
- Generation quality
- Training efficiency
- Parameter count

## ğŸ¤ Contributing

Contributions are welcome! Areas that could use help:

- Implementing remaining architectures (Attention, Transformer, MiniGPT)
- Improving documentation and tutorials
- Adding visualization tools
- Enhancing training efficiency
- Creating more comprehensive evaluation metrics

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
